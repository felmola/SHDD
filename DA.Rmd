---
title: ' Discriminant Analysis'
author: "Felipe Montealegre"
date: "2022-10-23"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview of the strategy:

Suppose our client is a Wholesale Distributor that has information on current and past clients expenditure on different products categories. We are required to design a tool to discriminate between smaller clients (i.e., the “Horeca” channel) and bigger clients (i.e., the “Retail” channel) depending on the expenditure per category of products.
First, we present a general overview of the theoretical aspects of Discriminant Analysis techniques.
Then we present a general overview of the structure of the data. We show the frequency of observations across channels and the distributions of the variables of interest. We study the assumptions of the model (namely normality and homoskedasticity between classes), comment on them, and transform the data accordingly for the analysis.
After that, we perform discriminant analysis using Linear Discriminant Analysis, Quadratic Discriminant Analysis and Bayes Discriminant Analysis. We compare different measures of accuracy across models and recommend one of them based on the results.

```{r, echo=FALSE, message=FALSE}
################################################################################
# Data setup
################################################################################
#install.packages("tidyverse")
#install.packages("magrittr")
#install.packages("bookdown")

library(bookdown)
library(tidyverse)
library(magrittr) # https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html

data<-read_csv("wholesale_customers_data.csv")
names(data) %<>% tolower
data <- data %>%
  rename(class = channel,
         delicatessen = delicassen)

data <- data %>%
  mutate(class = as.factor(data$class),
         region = as.factor(data$region))

levels(data$class) <- list("Horeca"=1, "Retail"=2)
levels(data$region) <- list("Lisbon"=1, "Oporto"=2, "Other"=3 )

```

# Descriptive statistics

The table below contains the descriptive statistics of the dataset. [*** general comment on the min max means variances].

```{r, echo=FALSE,  message=FALSE}

library(pastecs)
stat.desc(data)

```

The categorical variables are class and region, both of them denoting the type of client (i.e., if the client is a hotel, restaurant, or café (“horeca”)) and the region where the client is based (i.e., Lisbon, Oporto, or other region).67% of the clients belong to the “Horeca” category, and 32% belong to the retail channel. 17% of the clients are located in Lisbon, 11 percent of them are located in Oporto, and the remaining 72% are located in a different location.

```{r, echo=FALSE, message=FALSE}
library(epiDisplay)
table(data$class)
tab1(data$class, sort.group = "decreasing", cum.percent = TRUE, graph = FALSE)
tab1(data$region, sort.group = "decreasing", cum.percent = TRUE, graph = FALSE)
class <- data %>% ggplot(aes(x=class)) +
  geom_bar()

region <- data %>% ggplot(aes(x=region)) +
  geom_bar()

library(gridExtra)
grid.arrange(class, region)

```

The covariates contain information about expenditure in monetary units across 6 different categories:

1. Fresh: annual spending on fresh products.
2. Milk: annual spending on milk products.
3. Grocery: annual spending on grocery products.
4. Frozen: annual spending on frozen products.
5. Detergents_paper: annual spending on detergents and paper products.
6. Delicatessen: annual spending (m.u.) on and delicatessen products.

The figure below displays the distributions of each of the covariates. In general, most clients spend between 0k and 30k monetary units a year on all categories. Some clients spend much more than the average, with annual expenditures ranging between 30k and 100k sin some categories. Expenditure on detergents and/or paper, frozen products, and delicatessen products ranges from 0k to 10k approximately. This is expected as they are either luxury goods or goods which frequency is not as high as basic needs food such as fresh products, milk and grocery products in general.

```{r, echo=FALSE,  message=FALSE}
#------ frecuency of covariates:

library(scales)

suffix <- "k"
scale <- 1e-3

fresh <- ggplot(data, aes(x=fresh)) +
  geom_histogram() +
  scale_x_continuous(labels = label_number(suffix = suffix, scale = scale))

milk <- ggplot(data, aes(x=milk)) +
  geom_histogram() +
  scale_x_continuous(labels = label_number(suffix = suffix, scale = scale))

grocery <- ggplot(data, aes(x=grocery)) +
  geom_histogram() +
  scale_x_continuous(labels = label_number(suffix = suffix, scale = scale))

frozen <- ggplot(data, aes(x=frozen)) +
  geom_histogram() +
  scale_x_continuous(labels = label_number(suffix = suffix, scale = scale))

detergents_paper <- ggplot(data, aes(x=detergents_paper)) +
  geom_histogram() +
  scale_x_continuous(labels = label_number(suffix = suffix, scale = scale))

delicatessen <- ggplot(data, aes(x=delicatessen)) +
  geom_histogram() +
  scale_x_continuous(labels = label_number(suffix = suffix, scale = scale))

grid.arrange(fresh, milk, grocery, frozen, detergents_paper, delicatessen)

#https://cran.r-project.org/web/packages/gridExtra/vignettes/arrangeGrob.html

```

The figure below condenses the distributional information across covariates for ease of interpretation.

```{r, echo=FALSE,  message=FALSE}
#------ density plot (overlapped plots):
# The successor to reshape2 is tidyr. The equivalent of melt() and dcast() are gather() and spread() respectively.

data %>%
  dplyr::select(fresh, milk, grocery, frozen, detergents_paper, delicatessen) %>% 
  gather() %>% 
  #head()
  ggplot(aes(x=value, colour=key)) +
  geom_density() +
  scale_x_continuous(labels = label_number(suffix = suffix, scale = scale))
```

# Testing the assumptions of the model on the data

LDA assumes that the observations come from a normally distributed DGP with constant variance between classes. This generates decision rules that are linear in the covariates (i.e., it is possible to discriminate observations by a line or a plane, depending on the number of classes). If the homoskedasticity assumption is violated, then it is better to apply QDA. QDA generates decision rules by weighting ***. Bayes ***

We first explore variance equality between individual covariates using Bartlett's test for equal variance across samples for individual variables. Then we perform Cai TT, Ma Z (2013) two sample test of equality of covariance matrices. Bartlett's test strongly rejects the null hypothesis of equality of variance between groups for each of the covariates. This gives us a hit that for each of the covariates, the distribution of the data is quite different across classes. For the Cai and Ma test we strongly reject the null hypothesis that the covariance matrices between classes is equal.

Bartlett's test:
```{r, echo=FALSE,  message=FALSE}

################################################################################
# Testing assumptions (before transforming)
################################################################################

#------ homoskedasticity:

# Bartlett's test for equal variance across samples for individual variables:

#install.packages("olsrr")
#install.packages("xtable")
library(olsrr)
library(xtable)

temp_data <- data

bart_fresh <- ols_test_bartlett(temp_data, "fresh", group_var = "class")
bart_milk <- ols_test_bartlett(temp_data, "milk", group_var = "class")
bart_grocery <- ols_test_bartlett(temp_data, "grocery", group_var = "class")
bart_frozen <- ols_test_bartlett(temp_data, "frozen", group_var = "class")
bart_detergents_paper <- ols_test_bartlett(temp_data, "detergents_paper", group_var = "class")
bart_delicatessen <- ols_test_bartlett(temp_data, "delicatessen", group_var = "class")

bart_names <- temp_data %>%
  dplyr::select(fresh, milk, grocery, frozen, detergents_paper, delicatessen) %>%
  names()

decimals <- 1000

bart_pvalues <- c(
  round(bart_fresh[[2]], decimals),
  round(bart_milk[[2]], decimals),
  round(bart_grocery[[2]], decimals),
  round(bart_frozen[[2]], decimals),
  round(bart_detergents_paper[[2]], decimals),
  round(bart_detergents_paper[[2]], decimals)
)

bart_std <- tibble(bart_names, bart_pvalues)
bart_std
```
 
 Cai TT, Ma Z test:
 
```{r, echo=FALSE,  message=FALSE}
# Cai TT, Ma Z (2013) two sample test of equality of covariance matrices:
# https://cran.r-project.org/web/packages/CovTools/CovTools.pdf
#install.packages("CovTools")
library(CovTools)

class_1 <- data %>%
  dplyr::filter(class == "Horeca") %>%
  dplyr::select(fresh, milk, grocery, frozen, detergents_paper, delicatessen)

class_2 <- data %>%
  dplyr::filter(class == "Retail") %>%
  dplyr::select(fresh, milk, grocery, frozen, detergents_paper, delicatessen)

CovTest2.2013Cai(as.matrix(class_1), as.matrix(class_2))

```

Next, we explore the normality assumptions. First, we compute the Shapiro-Wilk test of normality for each of the covariates individually. For all the individual covariates we strongly reject the null hypothesis of normality. We then perform a Generalized Shapiro-Wilk test for Multivariate Normality. We again strongly reject the null hypothesis of joint normality in the sample.

Shaphiro wilk test:

```{r, echo=FALSE,  message=FALSE}
#------ normality:

# Shaphiro-Wilk test of normality for individual covariates:

temp_data <- data

shap_fresh <- shapiro.test(temp_data$fresh)
shap_milk <- shapiro.test(temp_data$milk)
shap_grocery <- shapiro.test(temp_data$grocery)
shap_frozen <- shapiro.test(temp_data$frozen)
shap_detergents_paper <- shapiro.test(temp_data$detergents_paper)
shap_delicatessen <- shapiro.test(temp_data$delicatessen)

shap_names <- temp_data %>%
  dplyr::select(fresh, milk, grocery, frozen, detergents_paper, delicatessen) %>% 
  names()

decimals <- 1000

shap_pvalues <- c(
  round(shap_fresh[[2]], decimals),
  round(shap_milk[[2]], decimals),
  round(shap_grocery[[2]], decimals),
  round(shap_frozen[[2]], decimals),
  round(shap_detergents_paper[[2]], decimals),
  round(shap_detergents_paper[[2]], decimals)
)

shap_std <- tibble(shap_names, shap_pvalues)
shap_std

```

Generalized Shapiro-Wilk test

```{r, echo=FALSE,  message=FALSE}

# Generalized Shapiro-Wilk test for Multivariate Normality:

#install.packages("QuantPsyc")
#install.packages("mvShapiroTest")
#install.packages("mvnormtest")
library(QuantPsyc)
library(mvShapiroTest)
library(mvnormtest)

temp_data <- data %>% 
  dplyr::select(fresh, milk, grocery, frozen, detergents_paper, delicatessen)

temp_data %>%
  data.matrix() %>%
  mvShapiro.Test()

mvnormtest::mshapiro.test(t(temp_data))

mult.norm(temp_data)$mult.test

```

We decide to apply a log transformation and standardize the data for ease of manipulation and to comply with the model assumptions. It is worth emphasizing that QDA does not require the assumption of equality of covariances between classes, and that Bayes Discriminant rule does not require the assumption of normality in the data.
As expected, once transformed and standardized, the mean and variance of the transformed covariates are equal to 0 and 1 respectively.

```{r, echo=FALSE,  message=FALSE}
################################################################################
# Data transformation and standardization.
################################################################################

ln_fresh<-scale(log(data$fresh+1))
ln_milk<-scale(log(data$milk+1))
ln_grocery<-scale(log(data$grocery+1))
ln_frozen<-scale(log(data$frozen+1))
ln_detergents_paper<-scale(log(data$detergents_paper+1))
ln_delicatessen<-scale(log(data$delicatessen+1))

data <- cbind(data, ln_fresh, ln_milk, ln_grocery, ln_frozen, ln_detergents_paper, ln_delicatessen)

library(dplyr)

data %>%
  dplyr::select(ln_fresh, ln_milk, ln_grocery, ln_frozen, ln_detergents_paper, ln_delicatessen) %>%
  stat.desc()

```
The distribution of transformed variables now resembles a normal distribution:

```{r, echo=FALSE,  message=FALSE}
################################################################################
# Some visualizations and descriptive statistics on the transformed data
################################################################################

# Distribution of the transformed covariates:

suffix <- ""
scale <- 1

ln_fresh <- ggplot(data, aes(x=ln_fresh)) +
  geom_histogram() +
  scale_x_continuous(labels = label_number(suffix = suffix, scale = scale))

ln_milk <- ggplot(data, aes(x=ln_milk)) +
  geom_histogram() +
  scale_x_continuous(labels = label_number(suffix = suffix, scale = scale))

ln_grocery <- ggplot(data, aes(x=ln_grocery)) +
  geom_histogram() +
  scale_x_continuous(labels = label_number(suffix = suffix, scale = scale))

ln_frozen <- ggplot(data, aes(x=ln_frozen)) +
  geom_histogram() +
  scale_x_continuous(labels = label_number(suffix = suffix, scale = scale))

ln_detergents_paper <- ggplot(data, aes(x=ln_detergents_paper)) +
  geom_histogram() +
  scale_x_continuous(labels = label_number(suffix = suffix, scale = scale))

ln_delicatessen <- ggplot(data, aes(x=ln_delicatessen)) +
  geom_histogram() +
  scale_x_continuous(labels = label_number(suffix = suffix, scale = scale))

grid.arrange(ln_fresh, ln_milk, ln_grocery, ln_frozen, ln_detergents_paper, ln_delicatessen)

#------ density plot (overlapped plots) on transformed data:

temp_data <- data %>% 
  dplyr::select(ln_fresh, ln_milk, ln_grocery, ln_frozen, ln_detergents_paper, ln_delicatessen)

temp_data %>%
  gather() %>% 
  #head()
  ggplot(aes(x=value, colour=key)) +
  geom_density() +
  scale_x_continuous(labels = label_number(suffix = suffix, scale = scale))

```
Nonetheless, the same normality tests as before are performed on the transformed data. 
The proportion of observations in each class should be considered when creating the priors. If not and the probabilities of classification do not match the class proportions, the classification procedure will be erroneous. Even though for two out of the six variables the hypothesis of homoskedasticity is still dejected, the generalized two-sample test of equality of covariances strongly rejects the null hypothesis of homoskedasticity.

Bartlett’s test:

```{r, echo=FALSE,  message=FALSE}
################################################################################
# Testing assumptions  on the transformed data.
################################################################################

#------ homoskedasticity:

# Bartlett's test for equal variance across samples for individual variables:

#install.packages("olsrr")
library(olsrr)

temp_data <- data %>%
  dplyr::select(class, ln_fresh, ln_milk, ln_grocery, ln_frozen, ln_detergents_paper, ln_delicatessen)

bart_fresh <- ols_test_bartlett(temp_data, "ln_fresh", group_var = "class")
bart_milk <- ols_test_bartlett(temp_data, "ln_milk", group_var = "class")
bart_grocery <- ols_test_bartlett(temp_data, "ln_grocery", group_var = "class")
bart_frozen <- ols_test_bartlett(temp_data, "ln_frozen", group_var = "class")
bart_detergents_paper <- ols_test_bartlett(temp_data, "ln_detergents_paper", group_var = "class")
bart_delicatessen <- ols_test_bartlett(temp_data, "ln_delicatessen", group_var = "class")

bart_names <- temp_data %>%
  dplyr::select(ln_fresh, ln_milk, ln_grocery, ln_frozen, ln_detergents_paper, ln_delicatessen) %>%
  names()

decimals <- 3

bart_pvalues <- c(
  round(bart_fresh[[2]], decimals),
  round(bart_milk[[2]], decimals),
  round(bart_grocery[[2]], decimals),
  round(bart_frozen[[2]], decimals),
  round(bart_detergents_paper[[2]], decimals),
  round(bart_detergents_paper[[2]], decimals)
)

bart_std <- tibble(bart_names, bart_pvalues)
bart_std

```

Cai TT, Ma Z test:

```{r, echo=FALSE,  message=FALSE}
# Cai TT, Ma Z (2013) two sample test of equality of covariance matrices:
# https://cran.r-project.org/web/packages/CovTools/CovTools.pdf
#install.packages("CovTools")
library(CovTools)

class_1 <- data %>%
  dplyr::filter(class == "Horeca") %>%
  dplyr::select(ln_fresh, ln_milk, ln_grocery, ln_frozen, ln_detergents_paper, ln_delicatessen)

class_2 <- data %>%
  dplyr::filter(class == "Retail") %>%
  dplyr::select(ln_fresh, ln_milk, ln_grocery, ln_frozen, ln_detergents_paper, ln_delicatessen)

CovTest2.2013Cai(as.matrix(class_1), as.matrix(class_2))

```

A similar story is told for the normality tests. The hypothesis of normality, both at the individual variable level and as a joint distribution of the covariates is strongly rejected. In a nutshell, the data is neither normally distributed nor homoscedastic across classes.

Shaphiro wilk test:
```{r, echo=FALSE,  message=FALSE}
#------ normality:

# Shaphiro-Wilk test of normality for individual covariates:

temp_data <- data %>% 
  dplyr::select(class, ln_fresh, ln_milk, ln_grocery, ln_frozen, ln_detergents_paper, ln_delicatessen)

shap_fresh <- shapiro.test(temp_data$ln_fresh)
shap_milk <- shapiro.test(temp_data$ln_milk)
shap_grocery <- shapiro.test(temp_data$ln_grocery)
shap_frozen <- shapiro.test(temp_data$ln_frozen)
shap_detergents_paper <- shapiro.test(temp_data$ln_detergents_paper)
shap_delicatessen <- shapiro.test(temp_data$ln_delicatessen)

shap_names <- temp_data %>%
  dplyr::select(ln_fresh, ln_milk, ln_grocery, ln_frozen, ln_detergents_paper, ln_delicatessen) %>% 
  names()

decimals <- 1000

shap_pvalues <- c(
  round(shap_fresh[[2]], decimals),
  round(shap_milk[[2]], decimals),
  round(shap_grocery[[2]], decimals),
  round(shap_frozen[[2]], decimals),
  round(shap_detergents_paper[[2]], decimals),
  round(shap_detergents_paper[[2]], decimals)
)

shap_std <- tibble(shap_names, shap_pvalues)
shap_std


```

Generalized Shaphiro wilk test:
```{r, echo=FALSE,  message=FALSE}
# Generalized Shapiro-Wilk test for Multivariate Normality:

#install.packages("QuantPsyc")
#install.packages("mvShapiroTest")
#install.packages("mvnormtest")
library(QuantPsyc)
library(mvShapiroTest)
library(mvnormtest)

temp_data <- data %>% 
  dplyr::select(ln_fresh, ln_milk, ln_grocery, ln_frozen, ln_detergents_paper, ln_delicatessen)

temp_data %>%
  data.matrix() %>%
  mvShapiro.Test()

mvnormtest::mshapiro.test(t(temp_data))

mult.norm(temp_data)$mult.test
```

# Discriminant Analysis

The total number of observations are split into two groups: the "training" group and the "testing” group using a split ratio of 0.65. The training subsample is used to create the discriminant rules while the testing subset is used to determine the different accuracy measures. After splitting the data, it is verified that the proportions of observations in the two classes are maintained both in the training set and in the testing set. The decomposition proportions are presented below.

```{r, echo=FALSE,  message=FALSE}
################################################################################
################################################################################
# Discriminant Analisys
################################################################################

# [https://www.r-bloggers.com/2021/05/linear-discriminant-analysis-in-r/]
#install.packages("MASS")
library(MASS)

#==============================================================================
# Splitting the sample
#==============================================================================

#install.packages("caTools")
library(caTools)
set.seed(33)

split<-sample.split(data$class,SplitRatio=0.65)

training<-subset(data,split==TRUE)
testing<-subset(data,split==FALSE)

n_complete <- table(data$class)
prop_complete <- table(data$class)/length(data$class)

n_training <- table(training$class)
prop_training <- table(training$class)/length(training$class)

n_testing <- table(testing$class)
prop_testing <- table(testing$class)/length(testing$class)

#names(table(testing$class)/length(testing$class))
#prop_complete[[1]]

prop_names <- c(
  "Complete dataset",
  "Training dataset",
  "Testing dataset"
)

prop_values_1 <- c(
  prop_complete[[1]],
  prop_training[[1]],
  prop_testing[[1]]
)

n_values_1 <- c(
  n_complete[[1]],
  n_training[[1]],
  n_testing[[1]]
)

prop_values_2 <- c(
  prop_complete[[2]],
  prop_training[[2]],
  prop_testing[[2]]
)

n_values_2 <- c(
  n_complete[[2]],
  n_training[[2]],
  n_testing[[2]]
)

total <- c(
  n_values_1[1] + n_values_2[1],
  n_values_1[2] + n_values_2[2],
  n_values_1[3] + n_values_2[3]
)
prop_table <- tibble(prop_names, n_values_1, prop_values_1, n_values_2, prop_values_2, total)
prop_table


```
The assumptions analysis in the previous code suggested that Bayes DA was more suited for analyzing the data and that QDA better than LDA. The table below presents accuracy measures for each of the three methods for discriminating between classes. Note that these accuracy measures use the training subset. The discriminating method with the highest accuracy rate is LDA (95%) while QDA and BAYES have accuracy rates of 93.7% and 94.4% respectively. The false positive rate is quite low for all discriminating methods, all range between 3% and 5%. Note, however, that because the accuracy is based on the same observations that were used to compute the discriminant rules, this method of evaluation is naively optimistic.

```{r, echo=FALSE,  message=FALSE, results=FALSE}
################################################################################
# Training sets
################################################################################

#==============================================================================
# LDA on the transformed data (training set)
#==============================================================================
temp_data <- training

model<-lda(formula=class~region+ln_fresh+ln_milk+ln_grocery+ln_frozen+ln_detergents_paper+ln_delicatessen,data=temp_data)
model

#install.packages("biotools")
library(biotools)
pred<-predict(model,data=temp_data)$class

confmat<-confusionmatrix(temp_data$class, pred)
confmat

#Accuracy
accuracy <- sum(diag(prop.table(confmat)))

#Misclassification Rate:(FP+FN)/total
total<-sum(confmat)
misclassification_rate <- (confmat[1,2]+confmat[2,1])/total

#Sensitivity, True Positive Rate:TP/(True yes)
true_yes<-confmat[2,1]+confmat[2,2]
sensitivity <- confmat[2,2]/true_yes

#False Positive Rate: FP/(True no).
true_no<-confmat[1,1]+confmat[1,2]
fpr <- confmat[1,2]/true_no

#Specificity, True Negative Rate: TN/(True no).
specificity <- confmat[1,1]/true_no

#Precision: TP/(predicted yes).
predicted_yes<-confmat[1,2]+confmat[2,2]
precision <-confmat[2,2]/predicted_yes

#Prevalence: (True yes)/total.
prevalence <- true_yes/total

fit_scores_LDA_train <- c(accuracy, misclassification_rate, sensitivity, fpr, specificity, precision, prevalence)

#------ building up the table:

fit_names <- c("Accuracy: (TP+TN)/total",
               "Misclassification Rate: (FP+FN)/total",
               "Sensitivity, True Positive Rate: TP/(True yes)",
               "False Positive Rate: FP/(True no)",
               "Specificity, True Negative Rate: TN/(True no)",
               "Precision: TP/(predicted yes)",
               "Prevalence: (True yes)/total")


fit_table <- tibble(fit_names, fit_scores_LDA_train)
fit_table

#==============================================================================
# QDA on the transformed data (training set)
#==============================================================================

temp_data <- training

model<-qda(formula=class~region+ln_fresh+ln_milk+ln_grocery+ln_frozen+ln_detergents_paper+ln_delicatessen,data=temp_data)
model

#install.packages("biotools")
library(biotools)
pred<-predict(model,data=temp_data)$class

confmat<-confusionmatrix(temp_data$class, pred)
confmat

#Accuracy
accuracy <- sum(diag(prop.table(confmat)))

#Misclassification Rate:(FP+FN)/total
total<-sum(confmat)
misclassification_rate <- (confmat[1,2]+confmat[2,1])/total

#Sensitivity, True Positive Rate:TP/(True yes)
true_yes<-confmat[2,1]+confmat[2,2]
sensitivity <- confmat[2,2]/true_yes

#False Positive Rate: FP/(True no).
true_no<-confmat[1,1]+confmat[1,2]
fpr <- confmat[1,2]/true_no

#Specificity, True Negative Rate: TN/(True no).
specificity <- confmat[1,1]/true_no

#Precision: TP/(predicted yes).
predicted_yes<-confmat[1,2]+confmat[2,2]
precision <-confmat[2,2]/predicted_yes

#Prevalence: (True yes)/total.
prevalence <- true_yes/total

fit_scores_QDA_train <- c(accuracy, misclassification_rate, sensitivity, fpr, specificity, precision, prevalence)

#------ building up the table:

fit_table <- tibble(fit_names, fit_scores_LDA_train, fit_scores_QDA_train)
fit_table

#==============================================================================
# Naive Bayes on the transformed data (training set)
#==============================================================================
#https://rpubs.com/Subhalaxmi/742119
#install.packages("naivebayes")
library(naivebayes)

temp_data <- training

model<-naive_bayes(formula=class~region+ln_fresh+ln_milk+ln_grocery+ln_frozen+ln_detergents_paper+ln_delicatessen,data=temp_data)
model

#install.packages("biotools")
library(biotools)
pred<-predict(model,data=temp_data)

confmat<-table(Predicted = pred, Actual = temp_data$class)
confmat

#Accuracy
accuracy <- sum(diag(prop.table(confmat)))

#Misclassification Rate:(FP+FN)/total
total<-sum(confmat)
misclassification_rate <- (confmat[1,2]+confmat[2,1])/total

#Sensitivity, True Positive Rate:TP/(True yes)
true_yes<-confmat[2,1]+confmat[2,2]
sensitivity <- confmat[2,2]/true_yes

#False Positive Rate: FP/(True no).
true_no<-confmat[1,1]+confmat[1,2]
fpr <- confmat[1,2]/true_no

#Specificity, True Negative Rate: TN/(True no).
specificity <- confmat[1,1]/true_no

#Precision: TP/(predicted yes).
predicted_yes<-confmat[1,2]+confmat[2,2]
precision <-confmat[2,2]/predicted_yes

#Prevalence: (True yes)/total.
prevalence <- true_yes/total

fit_scores_BAYES_train <- c(accuracy, misclassification_rate, sensitivity, fpr, specificity, precision, prevalence)

#------ building up the table:

fit_table <- tibble(fit_names, fit_scores_LDA_train, fit_scores_QDA_train, fit_scores_BAYES_train)
fit_table


```


```{r, echo=FALSE,  message=FALSE}
fit_table
```

The cross-validation criterion uses the training subset we had previously defined. The table below presents the accuracy measures under the testing subset. The accuracy rate of LDA is around 86% while QDA and BAYES are the same at 87%. The misclassification rates range around 14% to 16% being the highest for the LDA. In general, the measures of accuracy are very close across classification methods which suggest that means between classes are quite different while variance within classes is low. Note also that the similarity of accuracy measures between QDA and BAYES could Indicate that the estimates of the variance/covariance matrices are accurate enough.

```{r, echo=FALSE,  message=FALSE, results=FALSE}
################################################################################
# Testing sets
################################################################################

#==============================================================================
# LDA on the transformed data (testing set)
#==============================================================================
temp_data <- testing

model<-lda(formula=class~region+ln_fresh+ln_milk+ln_grocery+ln_frozen+ln_detergents_paper+ln_delicatessen,data=temp_data)
model

#install.packages("biotools")
library(biotools)
pred<-predict(model,data=temp_data)$class

confmat<-confusionmatrix(temp_data$class, pred)
confmat

#Accuracy
accuracy <- sum(diag(prop.table(confmat)))

#Misclassification Rate:(FP+FN)/total
total<-sum(confmat)
misclassification_rate <- (confmat[1,2]+confmat[2,1])/total

#Sensitivity, True Positive Rate:TP/(True yes)
true_yes<-confmat[2,1]+confmat[2,2]
sensitivity <- confmat[2,2]/true_yes

#False Positive Rate: FP/(True no).
true_no<-confmat[1,1]+confmat[1,2]
fpr <- confmat[1,2]/true_no

#Specificity, True Negative Rate: TN/(True no).
specificity <- confmat[1,1]/true_no

#Precision: TP/(predicted yes).
predicted_yes<-confmat[1,2]+confmat[2,2]
precision <-confmat[2,2]/predicted_yes

#Prevalence: (True yes)/total.
prevalence <- true_yes/total

fit_scores_LDA_test <- c(accuracy, misclassification_rate, sensitivity, fpr, specificity, precision, prevalence)

#------ building up the table:

fit_names <- c("Accuracy: (TP+TN)/total",
               "Misclassification Rate: (FP+FN)/total",
               "Sensitivity, True Positive Rate: TP/(True yes)",
               "False Positive Rate: FP/(True no)",
               "Specificity, True Negative Rate: TN/(True no)",
               "Precision: TP/(predicted yes)",
               "Prevalence: (True yes)/total")


fit_table <- tibble(fit_names, fit_scores_LDA_test)
fit_table

#==============================================================================
# QDA on the transformed data (testing set)
#==============================================================================

temp_data <- testing

model<-qda(formula=class~region+ln_fresh+ln_milk+ln_grocery+ln_frozen+ln_detergents_paper+ln_delicatessen,data=temp_data)
model

#install.packages("biotools")
library(biotools)
pred<-predict(model,data=temp_data)$class

confmat<-confusionmatrix(temp_data$class, pred)
confmat

#Accuracy
accuracy <- sum(diag(prop.table(confmat)))

#Misclassification Rate:(FP+FN)/total
total<-sum(confmat)
misclassification_rate <- (confmat[1,2]+confmat[2,1])/total

#Sensitivity, True Positive Rate:TP/(True yes)
true_yes<-confmat[2,1]+confmat[2,2]
sensitivity <- confmat[2,2]/true_yes

#False Positive Rate: FP/(True no).
true_no<-confmat[1,1]+confmat[1,2]
fpr <- confmat[1,2]/true_no

#Specificity, True Negative Rate: TN/(True no).
specificity <- confmat[1,1]/true_no

#Precision: TP/(predicted yes).
predicted_yes<-confmat[1,2]+confmat[2,2]
precision <-confmat[2,2]/predicted_yes

#Prevalence: (True yes)/total.
prevalence <- true_yes/total

fit_scores_QDA_test <- c(accuracy, misclassification_rate, sensitivity, fpr, specificity, precision, prevalence)

#------ building up the table:

fit_table <- tibble(fit_names, fit_scores_LDA_test, fit_scores_QDA_test)
fit_table

#==============================================================================
# NAIVE BAYES on the transformed data (testing set)
#==============================================================================
#https://rpubs.com/Subhalaxmi/742119
#install.packages("naivebayes")
library(naivebayes)

temp_data <- testing

model<-naive_bayes(formula=class~region+ln_fresh+ln_milk+ln_grocery+ln_frozen+ln_detergents_paper+ln_delicatessen,data=temp_data)
model

#install.packages("biotools")
library(biotools)
pred<-predict(model,data=temp_data)

confmat<-table(Predicted = pred, Actual = temp_data$class)
confmat

#Accuracy
accuracy <- sum(diag(prop.table(confmat)))

#Misclassification Rate:(FP+FN)/total
total<-sum(confmat)
misclassification_rate <- (confmat[1,2]+confmat[2,1])/total

#Sensitivity, True Positive Rate:TP/(True yes)
true_yes<-confmat[2,1]+confmat[2,2]
sensitivity <- confmat[2,2]/true_yes

#False Positive Rate: FP/(True no).
true_no<-confmat[1,1]+confmat[1,2]
fpr <- confmat[1,2]/true_no

#Specificity, True Negative Rate: TN/(True no).
specificity <- confmat[1,1]/true_no

#Precision: TP/(predicted yes).
predicted_yes<-confmat[1,2]+confmat[2,2]
precision <-confmat[2,2]/predicted_yes

#Prevalence: (True yes)/total.
prevalence <- true_yes/total

fit_scores_BAYES_test <- c(accuracy, misclassification_rate, sensitivity, fpr, specificity, precision, prevalence)

#------ building up the table:

fit_table <- tibble(fit_names, fit_scores_LDA_test, fit_scores_QDA_test, fit_scores_BAYES_test)
fit_table


```

```{r, echo=FALSE,  message=FALSE}

fit_table

```

The partitioning rules for each of the partitioning methods are presented below. The boundary between the pink and light blue areas is the partioning rule. The observations marked in Blue are correctly classfied. The observations in red are missclassified. "H" observations are the ones that belong to the "Horeca" class, while "R" observations belong to the "Retail" class.

**LDA:**

```{r, echo=FALSE,  message=FALSE}
#==============================================================================
# Visualization of partition rules
#==============================================================================
#install.packages("klaR")
library(klaR)

partimat(class~ln_fresh+ln_milk+ln_grocery+ln_frozen+ln_detergents_paper+ln_delicatessen, data = testing, method = "lda",
         col.mean = "black",
         col.correct="blue",
         col.wrong="firebrick3",
)


```

**QDA:**

```{r, echo=FALSE,  message=FALSE}
partimat(class~ln_fresh+ln_milk+ln_grocery+ln_frozen+ln_detergents_paper+ln_delicatessen, data = testing, method = "qda",
         col.mean = "black",
         col.correct="blue",
         col.wrong="firebrick3",
)
```

**Bayes:**

```{r, echo=FALSE,  message=FALSE}
partimat(class~ln_fresh+ln_milk+ln_grocery+ln_frozen+ln_detergents_paper+ln_delicatessen, data = testing, method = "naiveBayes",
         col.mean = "black",
         col.correct="blue",
         col.wrong="firebrick3",
)

```

# Additional Analisys

We applied the three methods for discriminating observations to the raw data. This is an additional measure of the efficiency of the models and gives us a hint wheter transforming and standardizing the data affects the results in any way. The table below shows the accuracy rates for the raw data.


```{r, echo=FALSE,  message=FALSE, results=FALSE}
################################################################################
# Testing sets [untransformed data]
################################################################################

#==============================================================================
# LDA on the untransformed data (testing set)
#==============================================================================
temp_data <- testing

model<-lda(formula=class~region+fresh+milk+grocery+frozen+detergents_paper+delicatessen,data=temp_data)
model

#install.packages("biotools")
library(biotools)
pred<-predict(model,data=temp_data)$class

confmat<-confusionmatrix(temp_data$class, pred)
confmat

#Accuracy
accuracy <- sum(diag(prop.table(confmat)))

#Misclassification Rate:(FP+FN)/total
total<-sum(confmat)
misclassification_rate <- (confmat[1,2]+confmat[2,1])/total

#Sensitivity, True Positive Rate:TP/(True yes)
true_yes<-confmat[2,1]+confmat[2,2]
sensitivity <- confmat[2,2]/true_yes

#False Positive Rate: FP/(True no).
true_no<-confmat[1,1]+confmat[1,2]
fpr <- confmat[1,2]/true_no

#Specificity, True Negative Rate: TN/(True no).
specificity <- confmat[1,1]/true_no

#Precision: TP/(predicted yes).
predicted_yes<-confmat[1,2]+confmat[2,2]
precision <-confmat[2,2]/predicted_yes

#Prevalence: (True yes)/total.
prevalence <- true_yes/total

fit_scores_LDA_test <- c(accuracy, misclassification_rate, sensitivity, fpr, specificity, precision, prevalence)

#------ building up the table:

fit_names <- c("Accuracy: (TP+TN)/total",
               "Misclassification Rate: (FP+FN)/total",
               "Sensitivity, True Positive Rate: TP/(True yes)",
               "False Positive Rate: FP/(True no)",
               "Specificity, True Negative Rate: TN/(True no)",
               "Precision: TP/(predicted yes)",
               "Prevalence: (True yes)/total")


fit_table <- tibble(fit_names, fit_scores_LDA_test)
fit_table

#==============================================================================
# QDA on the untransformed data (testing set)
#==============================================================================

temp_data <- testing

model<-qda(formula=class~region+fresh+milk+grocery+frozen+detergents_paper+delicatessen,data=temp_data)
model

#install.packages("biotools")
library(biotools)
pred<-predict(model,data=temp_data)$class

confmat<-confusionmatrix(temp_data$class, pred)
confmat

#Accuracy
accuracy <- sum(diag(prop.table(confmat)))

#Misclassification Rate:(FP+FN)/total
total<-sum(confmat)
misclassification_rate <- (confmat[1,2]+confmat[2,1])/total

#Sensitivity, True Positive Rate:TP/(True yes)
true_yes<-confmat[2,1]+confmat[2,2]
sensitivity <- confmat[2,2]/true_yes

#False Positive Rate: FP/(True no).
true_no<-confmat[1,1]+confmat[1,2]
fpr <- confmat[1,2]/true_no

#Specificity, True Negative Rate: TN/(True no).
specificity <- confmat[1,1]/true_no

#Precision: TP/(predicted yes).
predicted_yes<-confmat[1,2]+confmat[2,2]
precision <-confmat[2,2]/predicted_yes

#Prevalence: (True yes)/total.
prevalence <- true_yes/total

fit_scores_QDA_test <- c(accuracy, misclassification_rate, sensitivity, fpr, specificity, precision, prevalence)

#------ building up the table:

fit_table <- tibble(fit_names, fit_scores_LDA_test, fit_scores_QDA_test)
fit_table

#==============================================================================
# NAIVE BAYES on the untransformed data (testing set)
#==============================================================================
#https://rpubs.com/Subhalaxmi/742119
#install.packages("naivebayes")
library(naivebayes)

temp_data <- testing

model<-naive_bayes(formula=class~region+fresh+milk+grocery+frozen+detergents_paper+delicatessen,data=temp_data)
model

#install.packages("biotools")
library(biotools)
pred<-predict(model,data=temp_data)

confmat<-table(Predicted = pred, Actual = temp_data$class)
confmat

#Accuracy
accuracy <- sum(diag(prop.table(confmat)))

#Misclassification Rate:(FP+FN)/total
total<-sum(confmat)
misclassification_rate <- (confmat[1,2]+confmat[2,1])/total

#Sensitivity, True Positive Rate:TP/(True yes)
true_yes<-confmat[2,1]+confmat[2,2]
sensitivity <- confmat[2,2]/true_yes

#False Positive Rate: FP/(True no).
true_no<-confmat[1,1]+confmat[1,2]
fpr <- confmat[1,2]/true_no

#Specificity, True Negative Rate: TN/(True no).
specificity <- confmat[1,1]/true_no

#Precision: TP/(predicted yes).
predicted_yes<-confmat[1,2]+confmat[2,2]
precision <-confmat[2,2]/predicted_yes

#Prevalence: (True yes)/total.
prevalence <- true_yes/total

fit_scores_BAYES_test <- c(accuracy, misclassification_rate, sensitivity, fpr, specificity, precision, prevalence)

#------ building up the table:

fit_table <- tibble(fit_names, fit_scores_LDA_test, fit_scores_QDA_test, fit_scores_BAYES_test)
fit_table


```

```{r, echo=FALSE,  message=FALSE}
fit_table

```

```{r, echo=FALSE,  message=FALSE}


```




# THE END

